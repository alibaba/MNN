#include "opencl_source_map.hpp" 
namespace MNN { 
#ifndef MNN_OPENCL_BUFFER_CLOSED
const char* linear_attention_buf = 
"#ifdef MNN_SUPPORT_FP16\n"
"#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n"
"#endif\n"
"// Kernel 1: Depthwise Conv1D+SiLU\n"
"// Each work-item processes one (batch*channel,seq_pos) element.\n"
"// Input: qkv [B,D,L],conv_state [B,D,conv_state_size],conv_weight [D,1,K]\n"
"// Output: conv_out [B,D,L]\n"
"// conv_state is read but NOT updated here (updated by separate kernel).\n"
"__kernel void linear_attn_conv_silu(\n"
" __private const int global_dim0,\n"
" __global const FLOAT* qkv,\n"
" __global const FLOAT* conv_state,\n"
" __global const FLOAT* conv_weight,\n"
" __global FLOAT* conv_out,\n"
" __private const int batch,\n"
" __private const int conv_dim,\n"
" __private const int seq_len,\n"
" __private const int kernel_size,\n"
" __private const int conv_state_size)\n"
"{\n"
" const int gid=get_global_id(0);\n"
" if (gid >= global_dim0) return;\n"
" const int L=seq_len;\n"
" const int D=conv_dim;\n"
" const int K=kernel_size;\n"
" const int css=conv_state_size;\n"
" // Decompose: gid -> (batch_chan,seq_pos)\n"
" const int l=gid % L;\n"
" const int bd=gid/L;\n"
" const int b=bd/D;\n"
" const int d=bd % D;\n"
" // Compute valid convolution for position l\n"
" // Padded input=[conv_state[b,d,:],qkv[b,d,:]]\n"
" // padded[pos]: if pos<css -> conv_state[b*D*css+d*css+pos]\n"
" // else -> qkv[b*D*L+d*L+(pos-css)]\n"
" float sum=0.0f;\n"
" for (int k=0; k<K; ++k) {\n"
" int pos=l+k;\n"
" float input_val;\n"
" if (pos<css) {\n"
" input_val=(float)conv_state[b*D*css+d*css+pos];\n"
" } else {\n"
" input_val=(float)qkv[b*D*L+d*L+(pos-css)];\n"
" }\n"
" sum += input_val*(float)conv_weight[d*K+k];\n"
" }\n"
" // SiLU activation: x*sigmoid(x)\n"
" float sigmoid_val=1.0f/(1.0f+exp(-sum));\n"
" conv_out[b*D*L+d*L+l]=(FLOAT)(sum*sigmoid_val);\n"
"}\n"
"// Kernel 2: Update conv state with last (K-1) elements of padded input\n"
"// new_state[i]=padded[L+i],where padded=cat(old_state[css],qkv[L])\n"
"// Must execute AFTER linear_attn_conv_silu (which reads conv_state).\n"
"__kernel void linear_attn_conv_state_update(\n"
" __private const int global_dim0,\n"
" __global const FLOAT* qkv,\n"
" __global FLOAT* conv_state,\n"
" __private const int batch,\n"
" __private const int conv_dim,\n"
" __private const int seq_len,\n"
" __private const int conv_state_size)\n"
"{\n"
" const int gid=get_global_id(0);\n"
" if (gid >= global_dim0) return;\n"
" const int L=seq_len;\n"
" const int D=conv_dim;\n"
" const int css=conv_state_size;\n"
" const int i=gid % css;\n"
" const int bd=gid/css;\n"
" const int b=bd/D;\n"
" const int d=bd % D;\n"
" // new_state[i]=padded[L+i]\n"
" // padded=cat(old_state[css],qkv[L])\n"
" // position (L+i) in padded: if (L+i)<css -> old_state,else -> qkv[(L+i)-css]\n"
" int pos=L+i;\n"
" FLOAT val;\n"
" if (pos<css) {\n"
" val=conv_state[b*D*css+d*css+pos];\n"
" } else {\n"
" val=qkv[b*D*L+d*L+(pos-css)];\n"
" }\n"
" // Safe: we write to index i,read from index (L+i) where L >= 1,so (L+i)>i always\n"
" conv_state[b*D*css+d*css+i]=val;\n"
"}\n"
"// Kernel 3: Gated Delta Rule (Steps 2-5 fused)\n"
"// Each work-item processes one (batch,head) pair across all timesteps.\n"
"// Uses float32 exclusively for numerical stability of the recurrence.\n"
"__kernel void linear_attn_gated_delta_rule(\n"
" __private const int global_dim0,\n"
" __global const FLOAT* conv_out,\n"
" __global const FLOAT* gate,\n"
" __global const FLOAT* beta_in,\n"
" __global FLOAT* recurrent_state,\n"
" __global FLOAT* attn_out,\n"
" __private const int batch,\n"
" __private const int conv_dim,\n"
" __private const int seq_len,\n"
" __private const int num_k_heads,\n"
" __private const int num_v_heads,\n"
" __private const int head_k_dim,\n"
" __private const int head_v_dim,\n"
" __private const int key_dim,\n"
" __private const int val_dim,\n"
" __private const int gqa_factor,\n"
" __private const int use_l2norm,\n"
" __private const float q_scale)\n"
"{\n"
" const int gid=get_global_id(0);\n"
" if (gid >= global_dim0) return;\n"
" const int D=conv_dim;\n"
" const int L=seq_len;\n"
" const int H=num_v_heads;\n"
" const int d_k=head_k_dim;\n"
" const int d_v=head_v_dim;\n"
" const int b=gid/H;\n"
" const int h=gid % H; // V-head index\n"
" const int k_head=h/gqa_factor; // GQA: corresponding K-head\n"
" // State pointer: recurrent_state layout [B,H,d_k,d_v]\n"
" const int state_offset=(b*H+h)*d_k*d_v;\n"
" // Process each timestep sequentially\n"
" for (int t=0; t<L; ++t) {\n"
" // Step 2: Extract q_t,k_t,v_t from conv_out (transpose on the fly)\n"
" // conv_out layout: [B,D,L],access: conv_out[b*D*L+channel*L+t]\n"
" float q_local[256]; // max d_k\n"
" float k_local[256];\n"
" float v_local[256]; // max d_v\n"
" const int conv_base=b*D*L;\n"
" for (int i=0; i<d_k; ++i) {\n"
" q_local[i]=(float)conv_out[conv_base+(k_head*d_k+i)*L+t];\n"
" k_local[i]=(float)conv_out[conv_base+(key_dim+k_head*d_k+i)*L+t];\n"
" }\n"
" for (int i=0; i<d_v; ++i) {\n"
" v_local[i]=(float)conv_out[conv_base+(2*key_dim+h*d_v+i)*L+t];\n"
" }\n"
" // Step 3: Optional L2 Normalization on q_t and k_t\n"
" if (use_l2norm) {\n"
" const float eps=1e-6f;\n"
" float sumSq=0.0f;\n"
" for (int i=0; i<d_k; ++i) sumSq += q_local[i]*q_local[i];\n"
" float invNorm=1.0f/sqrt(sumSq+eps);\n"
" for (int i=0; i<d_k; ++i) q_local[i] *= invNorm;\n"
" sumSq=0.0f;\n"
" for (int i=0; i<d_k; ++i) sumSq += k_local[i]*k_local[i];\n"
" invNorm=1.0f/sqrt(sumSq+eps);\n"
" for (int i=0; i<d_k; ++i) k_local[i] *= invNorm;\n"
" }\n"
" // Step 4: Scale q_t by 1/sqrt(d_k)\n"
" for (int i=0; i<d_k; ++i) q_local[i] *= q_scale;\n"
" // Step 5: Gated Delta Rule recurrence\n"
" float g_t=(float)gate[b*L*H+t*H+h];\n"
" float beta_t=(float)beta_in[b*L*H+t*H+h];\n"
" // 5.1 Decay: S=S*exp(g_t)\n"
" float decay_val=exp(g_t);\n"
" for (int i=0; i<d_k*d_v; ++i) {\n"
" recurrent_state[state_offset+i]=(FLOAT)((float)recurrent_state[state_offset+i]*decay_val);\n"
" }\n"
" // 5.2 Read: v_pred=S^T @ k_t\n"
" float v_pred[256]; // max d_v\n"
" for (int j=0; j<d_v; ++j) {\n"
" float s=0.0f;\n"
" for (int i=0; i<d_k; ++i) {\n"
" s += (float)recurrent_state[state_offset+i*d_v+j]*k_local[i];\n"
" }\n"
" v_pred[j]=s;\n"
" }\n"
" // 5.3 Delta: delta=beta_t*(v_t-v_pred)\n"
" float delta[256]; // max d_v\n"
" for (int j=0; j<d_v; ++j) {\n"
" delta[j]=beta_t*(v_local[j]-v_pred[j]);\n"
" }\n"
" // 5.4 Write: S += k_t @ delta^T (outer product)\n"
" for (int i=0; i<d_k; ++i) {\n"
" float k_val=k_local[i];\n"
" for (int j=0; j<d_v; ++j) {\n"
" recurrent_state[state_offset+i*d_v+j]=(FLOAT)((float)recurrent_state[state_offset+i*d_v+j]+k_val*delta[j]);\n"
" }\n"
" }\n"
" // 5.5 Query: o_t=S^T @ q_t\n"
" const int out_offset=(b*L+t)*H*d_v+h*d_v;\n"
" for (int j=0; j<d_v; ++j) {\n"
" float s=0.0f;\n"
" for (int i=0; i<d_k; ++i) {\n"
" s += (float)recurrent_state[state_offset+i*d_v+j]*q_local[i];\n"
" }\n"
" attn_out[out_offset+j]=(FLOAT)s;\n"
" }\n"
" } // end timestep\n"
"}\n"
;
#endif
}
